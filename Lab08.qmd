---
title: "PM 566: Lab 08"
author: "Tarun Mahesh"
execute:
  warning: false
format: 
  html:
    embed-resources: true
fig-width: 10
fig-height: 10
theme: sandstone
---

```{r}
library(parallel)
library(microbenchmark)
```

Write a faster version of each function and show that (1) the outputs are the same as the slow version, and (2) your version is faster.

# Problem 1: Vectorization

```{r}
fun1 <- function(n = 100, k = 4, lambda = 4) {
  x <- NULL
  
  for (i in 1:n){
    x <- rbind(x, rpois(k, lambda))    
  }
  
  return(x)
}

fun1alt <- function(n = 100, k = 4, lambda = 4) {
  x <- matrix(rpois(n*k, lambda), nrow = n, ncol = k)
  return(x)
}

set.seed(5)
fun1res <- fun1()

set.seed(5)
fun1altres <- fun1alt()

paste("Rows of fun1: ", dim(fun1res)[1])
paste("Columns of fun1: ", dim(fun1res)[2])
paste("Rows of fun1alt: ", dim(fun1altres)[1])
paste("Columns of fun1alt: ", dim(fun1altres)[2])

paste("Mean of fun1: ", mean(fun1res))
paste("SD of fun1: ", sd(fun1res))

paste("Mean of fun1alt: ", mean(fun1altres))
paste("SD of fun1alt: ", sd(fun1altres))

# Benchmarking
microbenchmark::microbenchmark(
  fun1(),
  fun1alt()
)
```

The mean is the same, the dimensions are the same, and fun1alt() is about 9.5 times faster than fun1.

# Problem 2: Vectorization

```{r}
# Data Generating Process (10 x 10,000 matrix)
set.seed(1234)
x <- matrix(rnorm(1e4), nrow=10)

# Find each column's max value
fun2 <- function(x) {
  apply(x, 2, max)
}

# Used google to find a pre-made function that performs this

fun2alt <- function(x) {
  matrixStats::colMaxs(x)
}

fun2res <- fun2(x)
fun2altres <- fun2alt(x)

paste("Are the two function outputs identical?:", identical(fun2res, fun2altres))

# Benchmarking
microbenchmark::microbenchmark(
  fun2(x),
  fun2alt(x)
)
```

Both results are identical and fun2alt is about 20 times faster than fun2.

# **Problem 3: Parallelization**

This function implements a serial version of the bootstrap. Edit this function to parallelize the `lapply` loop, using whichever method you prefer. Rather than specifying the number of cores to use, use the number given by the `ncpus` argument, so that we can test it with different numbers of cores later.

```{r}
my_boot <- function(dat, stat, R, ncpus = 1L) {
  
  # Getting the random indices
  n <- nrow(dat)
  idx <- matrix(sample.int(n, n*R, TRUE), nrow=n, ncol=R)
  
  # THIS FUNCTION NEEDS TO BE PARALELLIZED
  # EDIT THIS CODE:
  if (ncpus <= 1L) {
    ans <- lapply(seq_len(R), function(i) stat(dat[idx[, i], , drop = FALSE]))
  } else {
    cl <- makeCluster(ncpus)
    on.exit(stopCluster(cl), add = TRUE)

    clusterExport(cl, varlist = c("dat", "idx", "stat"), envir = environment())

    ans <- parLapply(cl, seq_len(R), function(i) {
      stat(dat[idx[, i], , drop = FALSE])
    })
  }

do.call(rbind, ans)
}

```

Once you have a version of the `my_boot()` function that runs on multiple cores, check that it provides accurate results by comparing it to a parametric model:

```{r}
# Bootstrap of an OLS
my_stat <- function(d) coef(lm(y ~ x, data=d))

# DATA SIM
set.seed(1)
n <- 500; R <- 1e4

x <- cbind(rnorm(n)); y <- x*5 + rnorm(n)

# Checking if we get something similar as lm
ans0 <- confint(lm(y~x))
ans1 <- my_boot(dat = data.frame(x, y), my_stat, R = R, ncpus = 2L)

# You should get something like this
paste("Parallelized Function: ")
t(apply(ans1, 2, quantile, c(.025,.975)))

paste('Linear Model: ')
ans0
```

Check whether your version actually goes faster when it’s run on multiple cores (since this might take a little while to run, we’ll use `system.time` and just run each version once, rather than `microbenchmark`, which would run each version 100 times, by default):

```{r}
system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 1L))
system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 2L))
```

The parallelized function has the same confidence statistics and is faster when 2 CPUs are used compared to when 1 is used.
